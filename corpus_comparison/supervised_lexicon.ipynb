{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c27430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def find_opinion_by_lexicon(sens, lexicon_file, rate):\n",
    "    find_opinion = []\n",
    "    score_dic = {}\n",
    "    with open(lexicon_file) as file_obj:\n",
    "        pri_list = json.load(file_obj)\n",
    "    for i, s in enumerate(sens):\n",
    "        score = 0\n",
    "        for ss in s:\n",
    "            if ss.lower() in pri_list or ss in pri_list:\n",
    "                score += 1\n",
    "            if ss.lower() in words_md:\n",
    "                score += 2\n",
    "        score_dic[i] = score\n",
    "    \n",
    "    sorted_dic = sorted(score_dic.items(), key=lambda item:item[1], reverse=True)\n",
    "    extract_length = int(len(sens)*rate)\n",
    "    for i, sd in enumerate(sorted_dic):\n",
    "        if i < extract_length:\n",
    "            find_opinion.append(sens[sd[0]])\n",
    "    return find_opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7adb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237\n"
     ]
    }
   ],
   "source": [
    "# load train set\n",
    "opinions = []\n",
    "for i in range(2, 15):\n",
    "    with open(str(i)+'_batch/opinion_token.json') as file_obj:\n",
    "        opinion = json.load(file_obj)\n",
    "        opinions.extend(opinion)\n",
    "    \n",
    "with open('1_batch/driver_token.json') as file_obj:\n",
    "    opinion = json.load(file_obj)\n",
    "    opinions.extend(opinion) \n",
    "    \n",
    "with open('1_batch/barrier_token.json') as file_obj:\n",
    "    opinion = json.load(file_obj)\n",
    "    opinions.extend(opinion)\n",
    "\n",
    "print(len(opinions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce19dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from utils import construct_dict\n",
    "import os\n",
    "\n",
    "if 'opinion_dic.json' in os.listdir():\n",
    "    with open('opinion_dic.json') as file_obj:\n",
    "        opinion_dic = json.load(file_obj)\n",
    "else:\n",
    "    opinion_dic = construct_dict(opinions)\n",
    "    \n",
    "if 'lexicon_dic.json' in os.listdir():\n",
    "    with open('lexicon_dic.json') as file_obj:\n",
    "        lexicon_dic = json.load(file_obj)\n",
    "else:\n",
    "    print('please run unsupervised_lexicon.ipynb first to get lexicon_dic.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff1fb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12889 words in lexicon_dic\n",
      "3205 words in opinion_dic\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "opinion_words = []\n",
    "opinion_freq = []\n",
    "lexicon_freq = []\n",
    "words_md = ['could', 'may', 'would', 'must', 'might', 'shall', 'ought', 'can']\n",
    "for od in opinion_dic.keys():\n",
    "    if od in lexicon_dic.keys():\n",
    "        opinion_words.append(od)\n",
    "        opinion_freq.append(opinion_dic[od])\n",
    "        lexicon_freq.append(lexicon_dic[od])\n",
    "print(len(lexicon_dic.keys()), 'words in lexicon_dic')\n",
    "print(len(opinion_dic.keys()), 'words in opinion_dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4870e85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recycling', 'incentive', 'product', 'EoL', 'scheme', 'collection', 'management', 'policy', 'industry', 'design', 'lack', 'economic', 'responsibility', 'regulation', 'waste', 'producer', 'government', 'business', 'recyclers', 'material', 'consumer', 'environmental', 'infrastructure', 'manufacturer', 'recycle', 'awareness', 'recovery', 'disposal', 'landfill', 'stewardship', 'stakeholder', 'raw', 'create', 'reduce', 'appropriate', 'Regulations', 'develop', 'improve', 'ensure', 'encourage', 'activity', 'economically', 'challenge', 'valuable', 'opportunity', 'benefit', 'new', 'sustainable', 'essential', 'hazardous', 'Manufacturers', 'need', 'efficient', 'avoid', 'effective', 'profitable', 'cost', 'long', 'circular', 'loop', 'issue', 'party', 'still', 'burden', 'environmentally', 'target', 'EOL', 'effort', 'make', 'proper', 'developing', 'reducing', 'promote', 'necessary', 'social', 'future', 'fully', 'uncertainty', 'viability', 'financial', 'introducing', 'job', 'robust', 'rare', 'investment', 'PV', 'mandatory', 'domestic', 'guarantee', 'driver', 'barrier', 'voluntary', 'help', 'economy', 'better', 'participant', 'unfavorable', 'installers', 'ban', 'competitive']\n"
     ]
    }
   ],
   "source": [
    "# Sort(Normalization(Opinion_freq) - Normalization(lexicon_freq)) to construct dictionary\n",
    "\n",
    "opinion_freq = np.array(opinion_freq).reshape(1, -1)\n",
    "normalizer_opinion = preprocessing.Normalizer().fit_transform(opinion_freq)\n",
    "normalizer_opinion = list(normalizer_opinion[0])\n",
    "\n",
    "lexicon_freq = np.array(lexicon_freq).reshape(1, -1)\n",
    "normalizer_lexicon = preprocessing.Normalizer().fit_transform(lexicon_freq)\n",
    "normalizer_lexicon = list(normalizer_lexicon[0])\n",
    "\n",
    "more_value = {}\n",
    "for i, w in enumerate(opinion_words):\n",
    "    more_value[w] = normalizer_opinion[i]-normalizer_lexicon[i]\n",
    "pri_words = sorted(more_value.items(), key=lambda item:item[1], reverse=True)\n",
    "pri_list_whole = []\n",
    "i = 0\n",
    "for p_set in pri_words:\n",
    "    if p_set[0] not in words_md and opinion_dic[p_set[0]]>1:\n",
    "        # print(i, p_set, opinion_dic[p_set[0]])\n",
    "        pri_list_whole.append(p_set[0])\n",
    "        i += 1\n",
    "        \n",
    "# The amount can be adjusted, here choose 100  \n",
    "pri_list = pri_list_whole[:100]\n",
    "# json_str = json.dumps(pri_list, indent=4)\n",
    "# with open('pri_200.json', 'w', encoding='utf-8') as json_file:\n",
    "    # json_file.write(json_str)\n",
    "print(pri_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60594f6a",
   "metadata": {},
   "source": [
    "# Load test set (can be replaced to other set/file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b031370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695 sentences in total\n",
      "153 are opinion\n",
      "0.22014388489208633\n"
     ]
    }
   ],
   "source": [
    "sens = []\n",
    "opinion_answer = []\n",
    "driver_answer = []\n",
    "barrier_answer = []\n",
    "\n",
    "with open('UNSW_answer_without_conclusion/sen_token.json') as file_obj:\n",
    "    sen = json.load(file_obj)\n",
    "    sens.extend(sen)\n",
    "with open('UNSW_answer_without_conclusion/driver_token.json') as file_obj:\n",
    "    driver = json.load(file_obj)\n",
    "    driver_answer.extend(driver)\n",
    "    opinion_answer.extend(driver)\n",
    "with open('UNSW_answer_without_conclusion/barrier_token.json') as file_obj:\n",
    "    barrier = json.load(file_obj)\n",
    "    barrier_answer.extend(barrier)\n",
    "    opinion_answer.extend(barrier)\n",
    "with open('UNSW_answer_2/sen_token.json') as file_obj:\n",
    "    sen = json.load(file_obj)\n",
    "    sens.extend(sen)\n",
    "with open('UNSW_answer_2/driver_token.json') as file_obj:\n",
    "    driver = json.load(file_obj)\n",
    "    driver_answer.extend(driver)\n",
    "    opinion_answer.extend(driver)\n",
    "with open('UNSW_answer_2/barrier_token.json') as file_obj:\n",
    "    barrier = json.load(file_obj)\n",
    "    barrier_answer.extend(barrier)\n",
    "    opinion_answer.extend(barrier)\n",
    "    \n",
    "print(str(len(sens)) + ' sentences in total')\n",
    "print(str(len(opinion_answer)) + ' are opinion')\n",
    "print(len(opinion_answer)/len(sens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f74653dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n"
     ]
    }
   ],
   "source": [
    "find_opinion = find_opinion_by_lexicon(sens, 'pri_100.json', 0.2)\n",
    "print(len(find_opinion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5363b8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall, F1_Score, Accuracy\n",
      "(0.4460431654676259, 0.40522875816993464, 0.4246575342465754, 0.758273381294964)\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_metric\n",
    "\n",
    "print('Precision, Recall, F1_Score, Accuracy')\n",
    "print(calculate_metric(find_opinion, opinion_answer, len(sens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab2d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
